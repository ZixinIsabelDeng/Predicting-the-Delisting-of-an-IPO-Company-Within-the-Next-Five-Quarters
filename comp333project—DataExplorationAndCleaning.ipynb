{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZixinIsabelDeng/Predicting-the-Delisting-of-an-IPO-Company-Within-the-Next-Five-Quarters/blob/zixin/comp333project%E2%80%94DataExplorationAndCleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Predicting the Delisting of an IPO Company Within the Next Five Quarters**"
      ],
      "metadata": {
        "id": "gPEEfAbeEw5P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxQAtTiNTGYQ"
      },
      "outputs": [],
      "source": [
        "# a list of library to import\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n"
      ],
      "metadata": {
        "id": "g6Sy3UWiYS76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the dynamic and often volatile realm of the stock market, the ability to predict the delisting of companies post-IPO (Initial Public Offering) presents a critical advantage for investors, regulatory bodies, and economic analysts. This study focuses on the development and implementation of a predictive model aimed at forecasting the potential delisting of non-SPAC (Special Purpose Acquisition Company) IPO companies within five quarters following their market debut"
      ],
      "metadata": {
        "id": "ag2awUSLGfMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The significance of this prediction**\n",
        "\n",
        "-Investor Decision-Making: Investors stand to gain crucial insights into potential risks associated with their current or future investments in IPO companies. Early prediction helps in mitigating losses and optimizing portfolio performance by avoiding companies with a higher likelihood of delisting.\n",
        "Regulatory Oversight: Regulators can enhance their monitoring and oversight of newly listed companies, potentially identifying and addressing issues before they lead to delisting, thus maintaining market integrity and investor trust.\n",
        "Market Stability: Understanding and predicting delisting trends can help stabilize market dynamics by providing all market participants with better data on the health and longevity of new entrants.\n"
      ],
      "metadata": {
        "id": "Kr95c7HbFUCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Challenges in Predicting Delisting**\n",
        "\n",
        "Predicting the delisting of non-SPAC IPO companies within such a short period poses several significant challenges:\n",
        "\n",
        "\n",
        "The first challenge is data quality and scarcity. Reliable historical data on newly public companies, especially those facing delisting, is limited. We search multiple websites but most of them only provide listing IPO information. Fortunately, We are lucky to find delisting company information on Barchart.com and Alpha vantage API.  However, we don't know if those data are reliable.\n",
        "\n",
        "\n",
        "The second challenges is related to external factors. The stock market is influenced by numerous, often unpredictable external factors like economic shifts, policy changes, and global events which can abruptly alter a company's trajectory. Our analysis is based on financial data but does not mention any information related to elements mentioned above.\n",
        "\n",
        "\n",
        "In terms of Model Complexity and Bias, developing a model that accurately reflects the complexities of the real world without inheriting or amplifying biases present in historical data is a formidable task.Our project focus on SVM model and didn't use any hyperparameter tuning. If we are able to evaluate more on model selection and goes deeper in machine learning aspect. Our model can be more reliable,\n",
        "\n",
        "How can we improve?\n",
        "\n",
        "Our delisting company have SPAC companies. Those are mean to delisted, this factors will affect the reliability of our ml model"
      ],
      "metadata": {
        "id": "gKx3eIL1Gzom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load the data"
      ],
      "metadata": {
        "id": "M1jWYoJJAuH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download API and read excel file\n",
        "\n",
        "%cd /content/\n",
        "df1= pd.read_excel('delisted_companies_data - Copy.xlsx')\n",
        "df2=pd.read_csv('updated_listed_new_companies_data.csv')"
      ],
      "metadata": {
        "id": "EgYUMfx5Yrth",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "7e0dff6e-3f18-4f2e-c313-1528b881d134"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3263b177f33d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'delisted_companies_data - Copy.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'updated_listed_new_companies_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Cleaning**\n",
        "**Step 1. Removing irrelevant columns **\n",
        "\n",
        "This will related to non-times series data. we don't need information like company name, maket stock exchange place, symbol. We also don't need future data like delisting IPO date."
      ],
      "metadata": {
        "id": "fQrliYyekeBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "zITrbyc0WYbH",
        "outputId": "67870d5a-399e-426e-93be-a9da5e0d4ab5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  symbol                                               name exchange  \\\n",
              "0   AAIN                    Arlington Asset Investment Corp     NYSE   \n",
              "1   AAQC              Accelerate Acquisition Corp - Class A     NYSE   \n",
              "2   ABGI                   ABG Acquisition Corp I - Class A   NASDAQ   \n",
              "3  ACACU  PLAYSTUDIOS Inc - Units (1 Ord Share Class A &...   NASDAQ   \n",
              "4  ACAMU  CarLotz Inc - Unit (1 Ordinary Class A & 0.33 ...   NASDAQ   \n",
              "\n",
              "     ipoDate delistingDate    status  sector  numEmployees  salesQ5  salesQ4  \\\n",
              "0 2021-07-19    2024-01-30  Delisted     NaN           NaN        0        0   \n",
              "1 2021-05-10    2022-12-15  Delisted     NaN           NaN        0        0   \n",
              "2 2021-02-17    2023-02-27  Delisted     NaN           NaN        0        0   \n",
              "3 2020-10-23    2021-06-21  Delisted     NaN           NaN        0        0   \n",
              "4 2019-02-22    2021-01-21  Delisted     NaN           NaN        0        0   \n",
              "\n",
              "   ...  opCashflowQ5  opCashflowQ4  opCashflowQ3  opCashflowQ2  opCashflowQ1  \\\n",
              "0  ...             0             0             0             0             0   \n",
              "1  ...          -850          -560          -290         -1500         -1260   \n",
              "2  ...          -370          -270          -190          -980          -870   \n",
              "3  ...             0             0             0             0             0   \n",
              "4  ...             0             0             0             0             0   \n",
              "\n",
              "   netCashflowQ5  netCashflowQ4  netCashflowQ3  netCashflowQ2  netCashflowQ1  \n",
              "0              0              0              0              0              0  \n",
              "1           -360           -280           -290            940           1170  \n",
              "2           -410           -310           -240            450            560  \n",
              "3              0              0              0              0              0  \n",
              "4              0              0              0              0              0  \n",
              "\n",
              "[5 rows x 38 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a3d062ac-0e5e-4ea8-ba3d-6f79b1549c94\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symbol</th>\n",
              "      <th>name</th>\n",
              "      <th>exchange</th>\n",
              "      <th>ipoDate</th>\n",
              "      <th>delistingDate</th>\n",
              "      <th>status</th>\n",
              "      <th>sector</th>\n",
              "      <th>numEmployees</th>\n",
              "      <th>salesQ5</th>\n",
              "      <th>salesQ4</th>\n",
              "      <th>...</th>\n",
              "      <th>opCashflowQ5</th>\n",
              "      <th>opCashflowQ4</th>\n",
              "      <th>opCashflowQ3</th>\n",
              "      <th>opCashflowQ2</th>\n",
              "      <th>opCashflowQ1</th>\n",
              "      <th>netCashflowQ5</th>\n",
              "      <th>netCashflowQ4</th>\n",
              "      <th>netCashflowQ3</th>\n",
              "      <th>netCashflowQ2</th>\n",
              "      <th>netCashflowQ1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AAIN</td>\n",
              "      <td>Arlington Asset Investment Corp</td>\n",
              "      <td>NYSE</td>\n",
              "      <td>2021-07-19</td>\n",
              "      <td>2024-01-30</td>\n",
              "      <td>Delisted</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AAQC</td>\n",
              "      <td>Accelerate Acquisition Corp - Class A</td>\n",
              "      <td>NYSE</td>\n",
              "      <td>2021-05-10</td>\n",
              "      <td>2022-12-15</td>\n",
              "      <td>Delisted</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-850</td>\n",
              "      <td>-560</td>\n",
              "      <td>-290</td>\n",
              "      <td>-1500</td>\n",
              "      <td>-1260</td>\n",
              "      <td>-360</td>\n",
              "      <td>-280</td>\n",
              "      <td>-290</td>\n",
              "      <td>940</td>\n",
              "      <td>1170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ABGI</td>\n",
              "      <td>ABG Acquisition Corp I - Class A</td>\n",
              "      <td>NASDAQ</td>\n",
              "      <td>2021-02-17</td>\n",
              "      <td>2023-02-27</td>\n",
              "      <td>Delisted</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-370</td>\n",
              "      <td>-270</td>\n",
              "      <td>-190</td>\n",
              "      <td>-980</td>\n",
              "      <td>-870</td>\n",
              "      <td>-410</td>\n",
              "      <td>-310</td>\n",
              "      <td>-240</td>\n",
              "      <td>450</td>\n",
              "      <td>560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ACACU</td>\n",
              "      <td>PLAYSTUDIOS Inc - Units (1 Ord Share Class A &amp;...</td>\n",
              "      <td>NASDAQ</td>\n",
              "      <td>2020-10-23</td>\n",
              "      <td>2021-06-21</td>\n",
              "      <td>Delisted</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ACAMU</td>\n",
              "      <td>CarLotz Inc - Unit (1 Ordinary Class A &amp; 0.33 ...</td>\n",
              "      <td>NASDAQ</td>\n",
              "      <td>2019-02-22</td>\n",
              "      <td>2021-01-21</td>\n",
              "      <td>Delisted</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 38 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a3d062ac-0e5e-4ea8-ba3d-6f79b1549c94')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a3d062ac-0e5e-4ea8-ba3d-6f79b1549c94 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a3d062ac-0e5e-4ea8-ba3d-6f79b1549c94');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-00a05138-2da5-44fd-b6a6-8c6f12f5f2b5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-00a05138-2da5-44fd-b6a6-8c6f12f5f2b5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-00a05138-2da5-44fd-b6a6-8c6f12f5f2b5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df1"
            }
          },
          "metadata": {},
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#drop column sector and nonemployees due to null data, name and symnbol due to irrelevent to the prediction task\n",
        "\n",
        "df1 = df1.drop(columns=['sector', 'numEmployees', 'exchange', 'name', 'ipoDate', 'symbol', 'delistingDate'])\n",
        "df2 = df2.drop(columns=['sector', 'numEmployees', 'exchange', 'name', 'symbol', 'ipoDate'])\n",
        "df2['status']='listed'"
      ],
      "metadata": {
        "id": "0tntDUEWcq6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step 2: Fix structural errors*\n",
        "In our webscrapping data set, the null data is replaced by 0 and noan. and we don't want that 0 to affect the accurancy of our model. so when change all of them to null"
      ],
      "metadata": {
        "id": "2Dm_1jvm0PNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2.replace('None',0, inplace=True)\n",
        "df1.replace('None',0, inplace=True)\n",
        "df1= df1.replace(np.nan,0)\n",
        "df2= df2.replace(np.nan,0)"
      ],
      "metadata": {
        "id": "Jpc3hvUHBOcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['status']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "cdn3JWGrZpEl",
        "outputId": "edaa687d-7696-43c2-a155-65431aae66d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'status'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3653\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'status'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-233-195067a96880>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'status'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3761\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3762\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3763\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'status'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3. perform an initial SVM result evaluation to give a starting point for comparation. later, we will see how this result improves."
      ],
      "metadata": {
        "id": "LkOAiiufhfIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.concat([df1,df2]).copy()"
      ],
      "metadata": {
        "id": "UThm7p16V__V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def machine_learning_check1(data, target, test_size=0.3):\n",
        "    # Exclude the 'ipoDate' column\n",
        "\n",
        "    # we need an encoding for ml model\n",
        "    data['status']=data['status'].map({'Delisted':1,'listed':0})\n",
        "\n",
        "\n",
        "    # Split the training and testing set\n",
        "    X = data.drop(columns=[target])\n",
        "    y = data[target]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=43)\n",
        "\n",
        "    # Define the model using XGBoost\n",
        "    pipeline = Pipeline([\n",
        "\n",
        "        ('classifier', SVC(kernel='rbf'))  # Using RBF kernel\n",
        "    ])\n",
        "\n",
        "    # Fit the pipeline\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"SVM Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "umqusMZygJxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "step 4: We don't need rows that has more than 50 percent of null values. Rows with a high number of missing values can significantly distort the outcomes of our analysis. If a substantial portion of the data is missing, any calculations or predictions made using this data are likely to be unreliable. By removing such rows, we ensure that the remaining dataset maintains a higher standard of data integrity and quality."
      ],
      "metadata": {
        "id": "2g4WF7C7Lq-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#delete the whole row if all the financial data are all null for df2 or zero for df,\n",
        "def create_mask(df_name, selected_data):\n",
        "    if df_name == 'df1':\n",
        "        mask_zero_or_null = (selected_data == 0).all(axis=1)\n",
        "    elif df_name == 'df2':\n",
        "        mask_zero_or_null = selected_data.isnull().all(axis=1)\n",
        "    else:\n",
        "        raise ValueError(\"DataFrame name not recognized.\")\n",
        "    return mask_zero_or_null\n",
        "\n",
        "\n",
        "def delete_zero_or_null(df_name,start_col,df_name_string):\n",
        "  start_index=df_name.columns.get_loc(start_col)\n",
        "  selected_data=df_name.iloc[:,start_index:]\n",
        "  mask_zero_or_null=create_mask(df_name_string, selected_data)\n",
        "  return df_name[~mask_zero_or_null]\n",
        "\n",
        "\n",
        "df1=delete_zero_or_null(df1,'salesQ1','df1')\n",
        "df2=delete_zero_or_null(df2,'salesQ5','df2')"
      ],
      "metadata": {
        "id": "xw5N7owT845g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#concate listed and delisted company\n",
        "df=pd.concat([df1,df2])"
      ],
      "metadata": {
        "id": "bNycpCUa7A27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['status']"
      ],
      "metadata": {
        "id": "iYWtFC71ZjGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#some missing value shown as 0, we convert it to null value first then filling them together\n",
        "def convert_zero(df_data,from_col,to_col):\n",
        "  f=df_data.columns.get_loc(from_col)\n",
        "  t=df_data.columns.get_loc(to_col)\n",
        "  selected=df_data.iloc[:,f:t+1]\n",
        "  df_data.iloc[:, f:t+1] = selected.replace(0, np.nan)\n",
        "  return df\n",
        "df=convert_zero(df,'salesQ5','netCashflowQ1')"
      ],
      "metadata": {
        "id": "uyBz-si2REgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#most of the null value coming from sales data\n",
        "(df.isnull().sum()/len(df)).sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "xgfxiy1pXarB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Initialize the LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the 'status' column\n",
        "#df['status'] = encoder.fit_transform(df['status'])\n",
        "df['status']"
      ],
      "metadata": {
        "id": "jMjp_7IrZKFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform sensistive analysis to determine which null data filling technique gives the best result\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def sensitivity_analysis(data,target,test_size=0.3):\n",
        "\n",
        "  #split training and testing test\n",
        "  X=data.drop(columns=target)\n",
        "\n",
        "  y=data[target]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=43)\n",
        "\n",
        "  #include three types of ways we wanna fill the null data\n",
        "  imputers={\n",
        "      'mean_imputation':SimpleImputer(strategy='mean'),\n",
        "      'median_imputation':SimpleImputer(strategy='median'),\n",
        "      'knn_imputation': KNNImputer(n_neighbors=5)\n",
        "  }\n",
        "  models = {\n",
        "        'Logistic Regression': LogisticRegression(solver='liblinear',random_state=42),\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=100,random_state=42),\n",
        "        'SVM': SVC(kernel='linear',random_state=42)\n",
        "    }\n",
        "\n",
        "  results = {model_name: {} for model_name in models}\n",
        "  imputer_average = {imputer_name: [] for imputer_name in imputers}\n",
        "\n",
        "  for model_name, model in models.items():\n",
        "    for imputer_name, imputer in imputers.items():\n",
        "      pipeline = Pipeline([\n",
        "                        ('scaler', StandardScaler()),\n",
        "                        ('imputer', imputer),\n",
        "                        ('classifier', model)\n",
        "      ])\n",
        "      pipeline.fit(X_train, y_train)\n",
        "      y_pred = pipeline.predict(X_test)\n",
        "      accuracy = accuracy_score(y_test, y_pred)\n",
        "      results[model_name][imputer_name] = accuracy\n",
        "      imputer_average[imputer_name].append(accuracy)\n",
        "\n",
        "  for model_name, imputers in results.items():\n",
        "    print(f\"Model: {model_name}\")\n",
        "    for imputer_name, acc in imputers.items():\n",
        "      print(f\"  {imputer_name}: Accuracy = {acc:.4f}\")\n",
        "      print(\"\\n\")\n",
        "\n",
        "\n",
        "  # Print average accuracies for each imputer\n",
        "  print(\"Average Accuracies for Each Imputer Across Models:\")\n",
        "  for imputer_name, accuracies in imputer_average.items():\n",
        "    average_accuracy = np.mean(accuracies)\n",
        "    print(f\"  {imputer_name}: Average Accuracy = {average_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sensitivity_analysis(df,'status',)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bp8cdG8cvgmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we are not able to perform filling missing value on object data, so we convert all of them to numeric data\n",
        "object_cols = df.select_dtypes(include=['object']).columns\n",
        "df[object_cols] = df[object_cols].apply(pd.to_numeric, errors='coerce')"
      ],
      "metadata": {
        "id": "lmJnW49Rl02U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "status=df['status'].copy()"
      ],
      "metadata": {
        "id": "lDV9ezZxc4ZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.drop(columns='status')"
      ],
      "metadata": {
        "id": "oVnhZUJpc8v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# as per result,  we select mean_imputation\n",
        "for i in df.columns:\n",
        "  mean=df[i].mean()\n",
        "  df[i]=df[i].fillna(mean)\n"
      ],
      "metadata": {
        "id": "lg4pJOhGj3GN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step 5: Filter out data outliers*\n",
        "\n",
        "we are using z-score to filter out outliers\n",
        "\n",
        "The z-score of an observation quantifies how many standard deviations the observation is from the mean of the dataset. It is calculated using the formula:\n",
        "\n",
        "𝑧\n",
        "=\n",
        "(\n",
        "𝑋\n",
        "−\n",
        "𝜇\n",
        ")\n",
        "𝜎\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑋 is the value of the observation.\n",
        "𝜇 is the mean of the dataset.\n",
        "𝜎 is the standard deviation of the dataset.\n"
      ],
      "metadata": {
        "id": "NReLZ36DOwz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we remove rows from a DataFrame where any numeric column has a Z-score greater than threshold=3, effectively filtering out statistical outliers. It returns a new DataFrame without these outlier rows,"
      ],
      "metadata": {
        "id": "ryTyPxIwOd_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "def remove_outliers_zscore(df, threshold=3):\n",
        "    \"\"\"\n",
        "    Removes rows from the DataFrame where any numeric column's value has a Z-score > threshold.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pandas DataFrame from which to remove outliers\n",
        "    - threshold: Z-score value above which a data point is considered an outlier (default: 3)\n",
        "\n",
        "    Returns:\n",
        "    - A new DataFrame with outliers removed.\n",
        "    \"\"\"\n",
        "    outlier_mask = np.zeros(len(df), dtype=bool)  # Initialize a mask for rows to remove\n",
        "\n",
        "    col=['status']\n",
        "    df[col]=df[col].astype(dtype='object')\n",
        "    for column in df.select_dtypes(include=[np.number]).columns:\n",
        "        # Calculate Z-scores for the column\n",
        "        zs = np.abs(stats.zscore(df[column], nan_policy='omit'))\n",
        "        # Update the mask to include outliers in the current column\n",
        "        outlier_mask |= (zs > threshold)\n",
        "    return df[~outlier_mask]\n",
        "\n",
        "\n",
        "df = remove_outliers_zscore(df)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vQYVEI6nSWPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*step 6: a final formatting*\n",
        "we make the datatype consistent and perform a ml result check to see how we improve the result"
      ],
      "metadata": {
        "id": "PmT-GL0pO1fJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ir8gIsnKa9Nk",
        "outputId": "7b0eca32-b05e-47dd-a449-622d2e19aff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "status                3035\n",
              "salesQ5                  0\n",
              "salesQ4                  0\n",
              "salesQ3                  0\n",
              "salesQ2                  0\n",
              "salesQ1                  0\n",
              "netIncomeQ5              0\n",
              "netIncomeQ4              0\n",
              "netIncomeQ3              0\n",
              "netIncomeQ2              0\n",
              "netIncomeQ1              0\n",
              "totalAssetQ5             0\n",
              "totalAssetQ4             0\n",
              "totalAssetQ3             0\n",
              "totalAssetQ2             0\n",
              "totalAssetQ1             0\n",
              "totalLiabilitiesQ5       0\n",
              "totalLiabilitiesQ4       0\n",
              "totalLiabilitiesQ3       0\n",
              "totalLiabilitiesQ2       0\n",
              "totalLiabilitiesQ1       0\n",
              "opCashflowQ5             0\n",
              "opCashflowQ4             0\n",
              "opCashflowQ3             0\n",
              "opCashflowQ2             0\n",
              "opCashflowQ1             0\n",
              "netCashflowQ5            0\n",
              "netCashflowQ4            0\n",
              "netCashflowQ3            0\n",
              "netCashflowQ2            0\n",
              "netCashflowQ1            0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['status']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9Hbdi2rcqaq",
        "outputId": "e1044e0d-4e24-45ac-a821-08b213221925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1       NaN\n",
              "2       NaN\n",
              "10      NaN\n",
              "13      NaN\n",
              "17      NaN\n",
              "       ... \n",
              "2301    NaN\n",
              "2302    NaN\n",
              "2303    NaN\n",
              "2304    NaN\n",
              "2305    NaN\n",
              "Name: status, Length: 3035, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Initialize the LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the 'status' column\n",
        "df['status'] = encoder.fit_transform(df['status'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a25qavJEcD5f",
        "outputId": "1aa6148f-8290-405c-ed60-cb19565e4b36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-200-49556ad9b4fe>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['status'] = encoder.fit_transform(df['status'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['status']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPT_95R2cXCs",
        "outputId": "d4af5ced-8e23-402f-9b9b-d3dcd213648f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      NaN\n",
              "1      NaN\n",
              "2      NaN\n",
              "3      NaN\n",
              "4      NaN\n",
              "        ..\n",
              "3030   NaN\n",
              "3031   NaN\n",
              "3032   NaN\n",
              "3033   NaN\n",
              "3034   NaN\n",
              "Name: status, Length: 3035, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.reset_index(drop=True,inplace=True)\n",
        "#change all of them to int\n",
        "# Convert DataFrame to nullable integer type\n",
        "\n",
        "\n",
        "\n",
        "machine_learning_check1(df, 'status', test_size=0.3)"
      ],
      "metadata": {
        "id": "TNXArs0AY2Rg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "8bd47189-e84b-40b7-e2b7-c02aaa0a93e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input y contains NaN.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-202-83ddc394c1c2>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmachine_learning_check1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'status'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-188-73a316dd1acf>\u001b[0m in \u001b[0;36mmachine_learning_check1\u001b[0;34m(data, target, test_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Fit the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    193\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     )\n\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1142\u001b[0m         \u001b[0mestimator_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_estimator_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m         \u001b[0m_ensure_no_complex_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"O\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input y contains NaN."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Transformation**\n",
        "\n",
        "This part of data tranformation includes data normalization and apply of three different scalar to compare machine learning result on encoded dataset."
      ],
      "metadata": {
        "id": "jDa7su6AQB59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "normalizer = Normalizer()\n",
        "\n",
        "# Separate the 'status' column\n",
        "status = df['status'].copy()\n",
        "df_numeric = df.drop(columns='status')\n",
        "\n",
        "# Scale the numerical data\n",
        "normalized_data = normalizer.fit_transform(df_numeric)\n",
        "df_scaled = pd.DataFrame(normalized_data , columns=df_numeric.columns)\n",
        "\n",
        "# Add the 'status' column back\n",
        "df_scaled['status'] = status\n",
        "\n"
      ],
      "metadata": {
        "id": "zFaEpsTqsDo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def machine_learning_check2(data, target, test_size=0.3):\n",
        "    \"\"\"\n",
        "    Trains and evaluates an SVM classifier with different scaling methods on the provided dataset.\n",
        "\n",
        "    This function splits the data into training and testing sets, then applies different scalers (MinMaxScaler, StandardScaler, and RobustScaler)\n",
        "    to the training data. It then trains an SVM classifier with an RBF kernel for each scaled version of the data. It evaluates each model\n",
        "    on the testing set and prints the accuracy for each scaler.\n",
        "\n",
        "    Parameters:\n",
        "    - data (pandas.DataFrame): The dataset containing features and the target variable.\n",
        "    - target (str): The name of the target column in `data` which should be predicted.\n",
        "    - test_size (float, optional): The proportion of the dataset to include in the test split. Default is 0.3.\n",
        "\n",
        "    Returns:\n",
        "    - None: This function does not return anything but prints the accuracy of the model for each scaling method.\n",
        "    \"\"\"\n",
        "    # Exclude the target column\n",
        "    X = data.drop(columns=[target])\n",
        "    y = data[target]\n",
        "\n",
        "    # Split the training and testing set\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=232)\n",
        "\n",
        "    # Scalers to be tested\n",
        "    scalers = {\n",
        "        'MinMaxScaler': MinMaxScaler(),\n",
        "        'StandardScaler': StandardScaler(),\n",
        "        'RobustScaler': RobustScaler()\n",
        "    }\n",
        "\n",
        "    # Loop through each scaler and test it\n",
        "    for scaler_name, scaler in scalers.items():\n",
        "        # Define the pipeline with scaling and the SVM classifier\n",
        "        pipeline = Pipeline([\n",
        "            ('scaler', scaler),  # Dynamic scaler from the dictionary\n",
        "            ('classifier', SVC(kernel='rbf'))  # Using RBF kernel\n",
        "        ])\n",
        "\n",
        "        # Fit the pipeline\n",
        "        pipeline.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = pipeline.predict(X_test)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(f\"{scaler_name} SVM Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "machine_learning_check2(df_scaled, 'status', test_size=0.3)\n"
      ],
      "metadata": {
        "id": "Iu1roz1ADdrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data scaling: we select standard scaler"
      ],
      "metadata": {
        "id": "giFdc2KVRags"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Scale the DataFrame without the 'status' column\n",
        "scaled_data = scaler.fit_transform(df_scaled)\n",
        "df_scaled = pd.DataFrame(scaled_data , columns=df_scaled.columns)\n"
      ],
      "metadata": {
        "id": "MgPPqHFm7GTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_scaled['status'] = status"
      ],
      "metadata": {
        "id": "oLFPu0bUte61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "perform label encoding to scaled data and records ml scores"
      ],
      "metadata": {
        "id": "upON675WRv78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Initialize the LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the 'status' column\n",
        "df_scaled['status'] = encoder.fit_transform(df['status'])\n"
      ],
      "metadata": {
        "id": "rAv5KVKrs2QM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "machine_learning_check2(df_scaled, 'status', test_size=0.3)"
      ],
      "metadata": {
        "id": "tre5vFLWJH_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dimensionality Reduction**\n",
        "This part includes:\n",
        "\n",
        "*features selection*\n",
        "\n",
        "through correlation matric by selecting 0.85 as our threshold.\n",
        "\n",
        "*features transformation*\n",
        "\n",
        "through PCA by setting up the number of principal components as 18 as it can describe our data within 95% of variance by reducing more than 15 columns\n"
      ],
      "metadata": {
        "id": "_c926M77R8EY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_matrix = df_scaled.corr()\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# checking the correlation matrix\n",
        "plt.figure(figsize=(20, 20))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "plt.title('Correlation Matrix of Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qX_Ogbxd8Xqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part we wants to decide which columns to drop"
      ],
      "metadata": {
        "id": "8habrl6ITAA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.85\n",
        "\n",
        "#Find and remove the highly correlated features\n",
        "to_drop = set()\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
        "            col_name = correlation_matrix.columns[i]\n",
        "            to_drop.add(col_name)\n",
        "\n",
        "print(f\"Columns to drop: {to_drop}\")"
      ],
      "metadata": {
        "id": "cxvnyxYK9ZRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we compare SVM to check how much the result improves\n",
        "#before dropping\n",
        "\n",
        "machine_learning_check2(df_scaled, 'status', test_size=0.3)"
      ],
      "metadata": {
        "id": "HKuA78U0bsoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_scaled=df_scaled.drop(columns=['salesQ4', 'salesQ5', 'opCashflowQ3', 'totalAssetQ4'])"
      ],
      "metadata": {
        "id": "KaiHa7lo-vxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after dropping\n",
        "machine_learning_check2(df_scaled, 'status', test_size=0.3)\n"
      ],
      "metadata": {
        "id": "Tl4DG_3WbiMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*features transformation*\n",
        "\n",
        "since our ml accurancy does not improve too much. we decide to add anothe layer: PCA to captures 95% of variance and reducing the noices. we assume it will give us a better ml result"
      ],
      "metadata": {
        "id": "Oly8yp6jTZCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "pca.fit(df_scaled)\n",
        "\n",
        "# Get explained variance\n",
        "explained_variances = pca.explained_variance_ratio_"
      ],
      "metadata": {
        "id": "n9dwftClGzRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "how to select N principal component?   \n",
        "\n",
        "\n",
        "Initially, we decide to use elbow point to decide, however, elbow points only covers 50 percent of variance. this result if not idea. then we decide to first set an idea vairance coverage, then find the amount of PCA to achieve this result."
      ],
      "metadata": {
        "id": "UcKTfkWGT3aJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(explained_variances) + 1), explained_variances, 'o-', linewidth=2, color='blue')\n",
        "plt.title('Scree Plot')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "26ScxkMxmWNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize PCA with 18 components\n",
        "pca = PCA(n_components=18)\n",
        "\n",
        "# Fit PCA on the scaled data and transform it\n",
        "principalComponents = pca.fit_transform(df_scaled)\n",
        "\n",
        "# Create a DataFrame for the principal components\n",
        "principal_df = pd.DataFrame(\n",
        "    data=principalComponents,\n",
        "    columns=['PCA1', 'PCA2', 'PCA3', 'PCA4','PCA5','PCA6','PCA7','PCA8','PCA9','PCA10','PCA11','PCA12','PCA13','PCA14','PCA15','PCA16','PCA17','PCA18']\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ESdWoZJLm-5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "showing data after PCA tranformtion"
      ],
      "metadata": {
        "id": "ljxBFqtRUUka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "principal_df.head()"
      ],
      "metadata": {
        "id": "k1Bm4a2fQE0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the DataFrame with principal components\n",
        "\n",
        "principal_df['status']=status\n"
      ],
      "metadata": {
        "id": "bx3HzZCp99vF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration"
      ],
      "metadata": {
        "id": "tMNUOC0GE-Rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# we deliver this result to front end developer to perform dashboard visualization\n",
        "principal_df.to_csv('principal_componet.csv')"
      ],
      "metadata": {
        "id": "SPdFFYP_o5Uj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}